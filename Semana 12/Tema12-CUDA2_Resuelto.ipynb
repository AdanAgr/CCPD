{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1a81ee8c",
      "metadata": {
        "id": "1a81ee8c"
      },
      "source": [
        "# Tema 12: NVidia CUDA avanzado"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a64185",
      "metadata": {
        "id": "68a64185"
      },
      "source": [
        "## A - Warps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### EVITAR ERRORES\n",
        "\n",
        "!uv pip install -q --system numba-cuda==0.4.0\n",
        "\n",
        "from numba import config\n",
        "config.CUDA_ENABLE_PYNVJITLINK = 1"
      ],
      "metadata": {
        "id": "bP33OVmjD6NZ"
      },
      "id": "bP33OVmjD6NZ",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c46d58",
      "metadata": {
        "id": "76c46d58"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "threads_per_block = 1024\n",
        "blocks = 1024\n",
        "n = blocks*threads_per_block\n",
        "\n",
        "h_a = np.random.rand(n).astype(np.float32)\n",
        "d_a = cuda.to_device(h_a)\n",
        "\n",
        "@cuda.jit\n",
        "def experiment(a, convergence):\n",
        "    i = cuda.grid(1)\n",
        "    block_idx = cuda.blockIdx.x\n",
        "    if convergence == True:\n",
        "        if (block_idx%4==0):\n",
        "            a[i] = a[i]+2\n",
        "        elif (block_idx%4==1):\n",
        "            a[i] = np.sin(a[i])\n",
        "        elif (block_idx%4==2):\n",
        "            a[i] = np.cos(a[i])\n",
        "        elif (block_idx%4==3):\n",
        "            a[i] = a[i]**0.5\n",
        "    else:\n",
        "        if (i%4==0):\n",
        "            a[i] = a[i]+2\n",
        "        elif (i%4==1):\n",
        "            a[i] = np.sin(a[i])\n",
        "        elif (i%4==2):\n",
        "            a[i] = np.cos(a[i])\n",
        "        elif (i%4==3):\n",
        "            a[i] = a[i]**0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b1a1e1",
      "metadata": {
        "id": "37b1a1e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa994ae8-e999-4ec4-f412-0cf1aedf80ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "730 µs ± 47.1 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "1.04 ms ± 199 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit experiment[blocks, threads_per_block](d_a, True); cuda.synchronize()\n",
        "%timeit experiment[blocks, threads_per_block](d_a, False); cuda.synchronize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c095e11-95e6-4452-b3b5-a3eb0aa701ac",
      "metadata": {
        "id": "9c095e11-95e6-4452-b3b5-a3eb0aa701ac"
      },
      "source": [
        "## B - Acceso coalescente a memoria global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c65e7b6-b5ef-47e3-9b62-edc08f6383b3",
      "metadata": {
        "id": "3c65e7b6-b5ef-47e3-9b62-edc08f6383b3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "n = 1024*1024 # 10M\n",
        "threads_per_block = 1024\n",
        "blocks = 1024\n",
        "\n",
        "# Input Vectors of length 10M\n",
        "h_a = np.ones(n).astype(np.float32)\n",
        "h_b = h_a.copy().astype(np.float32)\n",
        "\n",
        "# Output Vector\n",
        "d_a = cuda.to_device(h_a)\n",
        "d_b = cuda.to_device(h_b)\n",
        "d_out = cuda.device_array_like(d_a)\n",
        "\n",
        "@cuda.jit\n",
        "def add_experiment(a, b, out, coalesced):\n",
        "    if coalesced == True:\n",
        "        i = cuda.grid(1)\n",
        "        out[i] = a[i] + b[i]\n",
        "    else:\n",
        "        thread_idx = cuda.threadIdx.x\n",
        "        block_idx = cuda.blockIdx.x\n",
        "        out[thread_idx*threads_per_block+block_idx] = \\\n",
        "            a[thread_idx*threads_per_block+block_idx] + \\\n",
        "            b[thread_idx*threads_per_block+block_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c416d5e1-5199-49c7-888d-2a5455185329",
      "metadata": {
        "id": "c416d5e1-5199-49c7-888d-2a5455185329"
      },
      "outputs": [],
      "source": [
        "%timeit add_experiment[blocks, threads_per_block](d_a, d_b, d_out, True); cuda.synchronize()\n",
        "%timeit add_experiment[blocks, threads_per_block](d_a, d_b, d_out, False); cuda.synchronize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e04607f",
      "metadata": {
        "id": "7e04607f"
      },
      "source": [
        "__Ejercicio: sumar filas y columnas de matriz__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca5134e",
      "metadata": {
        "id": "aca5134e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def row_sums(a, sums, n):\n",
        "    idx = cuda.grid(1)\n",
        "    sum = 0.0\n",
        "    for i in range(n):\n",
        "        sum += a[idx][i]\n",
        "    sums[idx] = sum\n",
        "\n",
        "@cuda.jit\n",
        "def col_sums(a, sums, n):\n",
        "    idx = cuda.grid(1)\n",
        "    sum = 0.0\n",
        "    for i in range(n):\n",
        "        sum += a[i][idx]\n",
        "    sums[idx] = sum\n",
        "\n",
        "n = 8192\n",
        "threads_per_block = 128\n",
        "blocks = n // threads_per_block  # 8192 / 128 = 64\n",
        "\n",
        "# Matriz de entrada\n",
        "h_a = np.ones((n, n), dtype=np.float32)\n",
        "\n",
        "# Copia a la GPU\n",
        "d_a = cuda.to_device(h_a)\n",
        "d_rows = cuda.device_array(n, dtype=np.float32)\n",
        "d_cols = cuda.device_array(n, dtype=np.float32)\n",
        "\n",
        "# Lanzamiento de kernels\n",
        "row_sums[blocks, threads_per_block](d_a, d_rows, n)\n",
        "col_sums[blocks, threads_per_block](d_a, d_cols, n)\n",
        "\n",
        "# Copiar resultados a CPU\n",
        "h_rows = d_rows.copy_to_host()\n",
        "h_cols = d_cols.copy_to_host()\n",
        "\n",
        "# Comprobación de resultados\n",
        "np.testing.assert_equal(h_rows, h_a.sum(axis=1))\n",
        "np.testing.assert_equal(h_cols, h_a.sum(axis=0))\n",
        "\n",
        "# Benchmark\n",
        "print(\"Tiempo suma de filas:\")\n",
        "%timeit row_sums[blocks, threads_per_block](d_a, d_rows, n); cuda.synchronize()\n",
        "print(\"Tiempo suma de columnas:\")\n",
        "%timeit col_sums[blocks, threads_per_block](d_a, d_cols, n); cuda.synchronize()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a129883",
      "metadata": {
        "id": "6a129883"
      },
      "source": [
        "## C - Kernels bidimensionales y tridimensionales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec93276b",
      "metadata": {
        "id": "ec93276b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def get_2D_indices(A):\n",
        "    x, y = cuda.grid(2) # Obtenemos las dos dimensiones\n",
        "    # Equivalente a:\n",
        "    # x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "    # y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
        "\n",
        "    # Escribimos índice x + '.' + índice y\n",
        "    A[x][y] = x + y / 10\n",
        "\n",
        "d_A = cuda.device_array(shape=(4,4), dtype=np.float32)\n",
        "    # Matriz 4x4 en la GPU\n",
        "\n",
        "blocks = (2, 2) # Grid = 2x2 bloques\n",
        "threads_per_block = (2, 2) # Bloque = 2x2 threads\n",
        "\n",
        "np.set_printoptions(precision=1, floatmode=\"fixed\")\n",
        "get_2D_indices[blocks, threads_per_block](d_A)\n",
        "\n",
        "print(d_A.copy_to_host())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6aaa06",
      "metadata": {
        "id": "0a6aaa06"
      },
      "source": [
        "__Ejercicio: sumar matrices con kernel 2D__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf804b2",
      "metadata": {
        "id": "eaf804b2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "n = 4096\n",
        "\n",
        "# TODO: completar\n",
        "@cuda.jit\n",
        "def matrix_add(a, b, out, coalesced):\n",
        "    x, y = cuda.grid(2)\n",
        "    if coalesced == True:\n",
        "      out[y][x] = a[y][x] + b[y][x]\n",
        "    else:\n",
        "      out[x][y] = a[x][y] + b[x][y]\n",
        "\n",
        "threads_per_block = (32, 32)  # 2D block\n",
        "blocks = (128, 128) # 2D grid\n",
        "\n",
        "h_a = np.arange(n*n).reshape(n,n).astype(np.float32)\n",
        "h_b = h_a.copy().astype(np.float32)\n",
        "\n",
        "# TODO: copia a la GPU y reserva para la salida\n",
        "\n",
        "d_a = cuda.to_device(h_a)\n",
        "d_b = cuda.to_device(h_b)\n",
        "d_out = cuda.device_array(shape=(n,n), dtype=np.float32)\n",
        "\n",
        "matrix_add[blocks, threads_per_block](d_a, d_b, d_out, True)\n",
        "h_out = d_out.copy_to_host()\n",
        "\n",
        "# TODO: invocación kernel y obtención resultados\n",
        "truth = h_a+h_b\n",
        "np.testing.assert_equal(h_out, truth)\n",
        "print(\"Coalescente\")\n",
        "%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, True); cuda.synchronize()\n",
        "print(\"No coalescente\")\n",
        "%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, False); cuda.synchronize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d18255",
      "metadata": {
        "id": "c4d18255"
      },
      "source": [
        "## D - Memoria compartida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7d864a",
      "metadata": {
        "id": "6d7d864a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numba import types, cuda\n",
        "\n",
        "n=5\n",
        "\n",
        "@cuda.jit\n",
        "def swap_with_shared(vector, swapped):\n",
        "    temp = cuda.shared.array(n, dtype=types.int32)\n",
        "    idx = cuda.grid(1)\n",
        "    temp[idx] = vector[idx]\n",
        "\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    swapped[idx] = temp[n-1 - cuda.threadIdx.x]\n",
        "\t\t# swap elements\n",
        "\n",
        "h_vector = np.arange(n).astype(np.int32)\n",
        "print(\"Antes:\",h_vector)\n",
        "h_swapped = np.zeros_like(h_vector)\n",
        "\n",
        "d_vector = cuda.to_device(h_vector)\n",
        "d_swapped = cuda.device_array(shape=(n,), dtype=np.int32)\n",
        "\n",
        "swap_with_shared[1, n](d_vector, d_swapped)\n",
        "result = d_swapped.copy_to_host()\n",
        "print(\"Después:\",result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "239de4db",
      "metadata": {
        "id": "239de4db"
      },
      "source": [
        "__Ejercicio: trasponer matriz con coalescencia usando memoria compartida__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "12f8d7fa",
      "metadata": {
        "id": "12f8d7fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b052894-b874-41a5-db49-f4eead857265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "139 µs ± 8.63 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "191 µs ± 7.84 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ],
      "source": [
        "from numba import cuda, types as numba_types\n",
        "import numpy as np\n",
        "\n",
        "n = 1024 * 1024  # 1M\n",
        "\n",
        "@cuda.jit\n",
        "def transpose(a, transposed):\n",
        "    x, y = cuda.grid(2)\n",
        "    if x < transposed.shape[0] and y < transposed.shape[1]:\n",
        "        transposed[x][y] = a[y][x]\n",
        "\n",
        "@cuda.jit\n",
        "def tile_transpose(a, transposed):\n",
        "    TILE_DIM = 32\n",
        "    tile = cuda.shared.array((TILE_DIM, TILE_DIM), numba_types.float32)\n",
        "\n",
        "    a_row, a_col = cuda.grid(2)\n",
        "\n",
        "    row = cuda.threadIdx.y\n",
        "    col = cuda.threadIdx.x\n",
        "\n",
        "    if a_row < a.shape[0] and a_col < a.shape[1]:\n",
        "        tile[row, col] = a[a_row, a_col]\n",
        "\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    t_row = cuda.blockIdx.x * TILE_DIM + row\n",
        "    t_col = cuda.blockIdx.y * TILE_DIM + col\n",
        "\n",
        "    if t_col < transposed.shape[0] and t_row < transposed.shape[1]:\n",
        "        transposed[t_col, t_row] = tile[col, row]\n",
        "\n",
        "threads_per_block = (32, 32)  # 2D blocks\n",
        "blocks = (1024 // 32, 1024 // 32)  # 2D grid = (32, 32)\n",
        "\n",
        "# 1024x1024 input and output matrices\n",
        "h_a = np.arange(n).reshape((1024, 1024)).astype(np.float32)\n",
        "d_a = cuda.to_device(h_a)\n",
        "d_transposed = cuda.device_array(shape=(1024, 1024), dtype=np.float32)\n",
        "\n",
        "# Invocación a traspose y comprobación\n",
        "transpose[blocks, threads_per_block](d_a, d_transposed)\n",
        "result = d_transposed.copy_to_host()\n",
        "expected = h_a.T\n",
        "np.testing.assert_equal(result, expected)\n",
        "\n",
        "# Invocación a tile_traspose y comprobación\n",
        "tile_transpose[blocks, threads_per_block](d_a, d_transposed)\n",
        "result = d_transposed.copy_to_host()\n",
        "expected = h_a.T\n",
        "np.testing.assert_equal(result, expected)\n",
        "\n",
        "%timeit transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()\n",
        "%timeit tile_transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0f65207",
      "metadata": {
        "id": "a0f65207"
      },
      "source": [
        "## F - Comparación accesos a memoria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cef806",
      "metadata": {
        "id": "c1cef806"
      },
      "outputs": [],
      "source": [
        "from numba import jit, cuda, types as numba_types\n",
        "import numpy as np\n",
        "\n",
        "n = 1024*1024 # 1M\n",
        "\n",
        "# ----------------------------\n",
        "\n",
        "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
        "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
        "\n",
        "@jit\n",
        "def transpose_CPU(a, transposed):\n",
        "    for i in range(1024):\n",
        "        for j in range(1024):\n",
        "            transposed[i,j] = a[j,i]\n",
        "\n",
        "transpose_CPU(h_a, h_out)\n",
        "expected = h_a.T\n",
        "np.testing.assert_equal(h_out, expected)\n",
        "\n",
        "# ----------------------------\n",
        "\n",
        "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
        "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
        "d_a = cuda.to_device(h_a)\n",
        "d_out = cuda.device_array(shape=(1024,1024), dtype=np.float32)\n",
        "\n",
        "@cuda.jit\n",
        "def transpose1thread(a, transposed):\n",
        "    for i in range(1024):\n",
        "        for j in range(1024):\n",
        "            transposed[i,j] = a[j,i]\n",
        "\n",
        "transpose1thread[1, 1](d_a, d_out)\n",
        "expected = h_a.T\n",
        "h_out=d_out.copy_to_host()\n",
        "np.testing.assert_equal(h_out, expected)\n",
        "\n",
        "# ----------------------------\n",
        "\n",
        "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
        "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
        "d_a = cuda.to_device(h_a)\n",
        "d_out = cuda.device_array(shape=(1024,1024), dtype=np.float32)\n",
        "\n",
        "@cuda.jit\n",
        "def transpose1024thread(a, transposed):\n",
        "    i = cuda.threadIdx.x\n",
        "    for j in range(1024):\n",
        "        transposed[i,j] = a[j,i]\n",
        "\n",
        "transpose1024thread[1, 1024](d_a, d_out)\n",
        "expected = h_a.T\n",
        "h_out=d_out.copy_to_host()\n",
        "np.testing.assert_equal(h_out, expected)\n",
        "\n",
        "# ----------------------------\n",
        "\n",
        "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
        "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
        "d_a = cuda.to_device(h_a)\n",
        "d_out = cuda.device_array(shape=(1024,1024), dtype=np.float32)\n",
        "\n",
        "@cuda.jit\n",
        "def transpose1Mthreads(a, transposed):\n",
        "    (i,j) = cuda.grid(2)\n",
        "    transposed[i,j] = a[j,i]\n",
        "\n",
        "blocks=(32,32)\n",
        "threads_per_block=(32,32)\n",
        "transpose1Mthreads[blocks, threads_per_block](d_a, d_out)\n",
        "h_out=d_out.copy_to_host()\n",
        "expected = h_a.T\n",
        "np.testing.assert_equal(h_out, expected)\n",
        "\n",
        "# ----------------------------\n",
        "\n",
        "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
        "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
        "d_a = cuda.to_device(h_a)\n",
        "d_out = cuda.device_array(shape=(1024,1024), dtype=np.float32)\n",
        "\n",
        "nthreads=32\n",
        "nblocks=int(1024/nthreads)\n",
        "@cuda.jit\n",
        "def tile_transpose(a, transposed):\n",
        "    tile = cuda.shared.array((nthreads, nthreads), numba_types.float32)\n",
        "    a_row, a_col = cuda.grid(2)\n",
        "    tile[cuda.threadIdx.x, cuda.threadIdx.y] = a[a_row, a_col]\n",
        "    cuda.syncthreads()\n",
        "    transposed[a_col, a_row] = tile[cuda.threadIdx.x, cuda.threadIdx.y]\n",
        "\n",
        "blocks=(nblocks,nblocks)\n",
        "threads_per_block=(nthreads, nthreads)\n",
        "tile_transpose[blocks, threads_per_block](d_a, d_out)\n",
        "h_out=d_out.copy_to_host()\n",
        "expected = h_a.T\n",
        "np.testing.assert_equal(h_out, expected)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "\n",
        "h_a = np.arange(n).reshape((1024,1024)).astype(np.float32)\n",
        "h_out = np.zeros(n).reshape((1024,1024)).astype(np.float32)\n",
        "d_a = cuda.to_device(h_a)\n",
        "d_out = cuda.device_array(shape=(1024,1024), dtype=np.float32)\n",
        "\n",
        "nthreads=32\n",
        "nblocks=int(1024/nthreads)\n",
        "ncols=33\n",
        "@cuda.jit\n",
        "def tile_transpose2(a, transposed):\n",
        "    tile = cuda.shared.array((nthreads, ncols), numba_types.float32)\n",
        "    a_row, a_col = cuda.grid(2)\n",
        "    tile[cuda.threadIdx.x, cuda.threadIdx.y] = a[a_row, a_col]\n",
        "    cuda.syncthreads()\n",
        "    transposed[a_col, a_row] = tile[cuda.threadIdx.x, cuda.threadIdx.y]\n",
        "\n",
        "blocks=(nblocks,nblocks)\n",
        "threads_per_block=(nthreads, nthreads)\n",
        "tile_transpose[blocks, threads_per_block](d_a, d_out)\n",
        "h_out=d_out.copy_to_host()\n",
        "expected = h_a.T\n",
        "np.testing.assert_equal(h_out, expected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc8b581",
      "metadata": {
        "id": "edc8b581"
      },
      "outputs": [],
      "source": [
        "%timeit transpose_CPU(h_a, h_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c354d545",
      "metadata": {
        "id": "c354d545"
      },
      "outputs": [],
      "source": [
        "%timeit transpose1thread[1, 1](d_a, d_out); cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ef6838",
      "metadata": {
        "id": "b6ef6838"
      },
      "outputs": [],
      "source": [
        "%timeit transpose1024thread[1, 1024](d_a, d_out); cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "639f8273",
      "metadata": {
        "id": "639f8273"
      },
      "outputs": [],
      "source": [
        "%timeit transpose1Mthreads[(32,32), (32,32)](d_a, d_out); cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cd1ed2f",
      "metadata": {
        "id": "1cd1ed2f"
      },
      "outputs": [],
      "source": [
        "%timeit tile_transpose[(32,32), (32,32)](d_a, d_out); cuda.synchronize()\n",
        "%timeit tile_transpose[(64,64), (16,16)](d_a, d_out); cuda.synchronize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2718139d",
      "metadata": {
        "id": "2718139d"
      },
      "outputs": [],
      "source": [
        "%timeit tile_transpose2[(32,32), (32,32)](d_a, d_out); cuda.synchronize()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}